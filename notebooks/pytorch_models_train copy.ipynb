{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruno\\anaconda3\\envs\\edap5\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       id                                             lyrics  \\\n",
      "0  1FAmKoufyAXMfzPPs9bsjA  i tied my bandana took my pack from the floor ...   \n",
      "1  3QvPEv8XjHa73iYhaienWw  i want to live on the moon never see a human a...   \n",
      "2  5VPFATm85G3P04Q5g8yxqr  bitch you know you can t parallel park anyway ...   \n",
      "3  7J2jCftItt7htcOUdcMnpt  graceless falling slipping in the cold with no...   \n",
      "4  4cBPzVIbDIQx0LIyauFAy0  madame morse estate stood five hundred years p...   \n",
      "\n",
      "       artist_name most_common_genre  \\\n",
      "0  Waylon Jennings           country   \n",
      "1   Phantom Planet              rock   \n",
      "2    Isaiah Rashad           hip-hop   \n",
      "3     Matt Pond PA             indie   \n",
      "4       Ariel Pink               pop   \n",
      "\n",
      "                                          genre_list  \n",
      "0  ['country', 'country', 'rock', 'outlaw', 'coun...  \n",
      "1                                    ['pop', 'rock']  \n",
      "2  ['hip-hop', 'rap', 'tennessee', 'hip-hop', 'un...  \n",
      "3                                ['philly', 'indie']  \n",
      "4  ['art', 'pop', 'chillwave', 'dream', 'pop', 'e...  \n"
     ]
    }
   ],
   "source": [
    "# Load train and test data\n",
    "train_data = pd.read_csv(\"../data/train_data.csv\")\n",
    "test_data = pd.read_csv(\"../data/test_data.csv\")\n",
    "\n",
    "print(train_data.head())\n",
    "\n",
    "# Split train and test data into features and targets\n",
    "train_features = train_data[\"lyrics\"]\n",
    "train_targets = train_data[\"most_common_genre\"]\n",
    "\n",
    "test_features = test_data[\"lyrics\"]\n",
    "test_targets = test_data[\"most_common_genre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151, 3, 28, 3, 22, 10, 7580, 1203, 852, 17, 22, 10, 23, 36, 724, 50, 27, 17, 22, 23, 2, 1416, 58, 34, 128, 132, 15, 20, 62, 9, 1182, 252, 1, 16, 62, 36, 724, 50, 58, 41, 253, 45, 3, 220, 4, 41, 253, 211, 3, 42, 293, 293, 293, 17, 22, 10, 23, 36, 724, 50, 27, 17, 22, 23, 2, 1416, 58, 34, 128, 132, 15, 20, 62, 9, 1182, 252, 1, 16, 62, 36, 724, 50, 58, 41, 253, 45, 3, 220, 4, 41, 253, 211, 3, 42, 293, 293, 293, 65, 37, 9, 595, 1081, 900, 4, 9, 976, 51, 6, 172, 50, 139, 51, 6, 172, 50, 403, 3, 1683, 1185, 92, 8, 76, 3, 84, 10627, 71, 34, 11, 18, 1327, 40, 7, 76, 3, 170, 10627, 71, 34, 11, 18, 217, 1, 80, 178, 98, 6, 371, 1, 22, 10, 45, 31, 15, 9, 226, 35, 36, 209, 5, 49, 4, 13, 11, 6, 143, 50, 162, 41, 253, 33, 3, 37, 3, 383, 41, 119, 10, 410, 3, 37, 3, 167, 46, 3, 251, 21, 6, 861, 13, 11, 2, 220, 50, 27, 17, 22, 23, 2, 1416, 58, 34, 128, 132, 15, 20, 62, 9, 1182, 252, 1, 16, 62, 36, 724, 50, 58, 41, 253, 45, 3, 220, 4, 41, 253, 211, 3, 42, 293, 293, 293, 17, 22, 10, 23, 36, 724, 50, 27, 17, 22, 23, 2, 1416, 58, 34, 128, 132, 15, 20, 62, 9, 1182, 252, 1, 16, 62, 36, 724, 50, 58, 41, 253, 45, 3, 220, 4, 41, 253, 211, 3, 42, 293, 293, 293, 65, 37, 9, 976, 1081, 597, 327, 3, 540, 21, 2, 826, 10627, 40, 7, 76, 3, 170, 10627, 329, 3, 498, 8, 19787, 540, 21, 2, 172, 634, 46, 1, 69, 7, 1185, 40, 7, 21, 2, 514, 634, 1, 22, 55, 18, 285, 244, 9, 623, 20, 3961, 42, 244, 18, 441, 757, 42, 47, 119, 10, 573, 2568, 70, 42, 2, 357, 327, 47, 124, 6, 5757, 5572, 901, 164, 2, 483, 1, 25, 10, 24, 5, 2774, 136, 1, 25, 10, 24, 5, 55, 2, 4506, 161, 8, 37, 3, 2811, 42, 1, 240, 259, 2, 982, 26, 8, 1, 240, 211, 2, 65, 4, 2065, 15, 9, 7218, 17, 22, 1574, 6, 288, 104, 439, 31, 20, 76, 7, 11, 877, 5, 23, 24, 17, 124, 6, 141, 4344, 130, 2, 4625, 130, 26, 8, 17, 15, 106, 226, 10627, 27, 17, 22, 23, 2, 1416, 58, 34, 128, 132, 15, 20, 62, 9, 1182, 252, 1, 16, 62, 36, 724, 50, 58, 41, 253, 45, 3, 220, 4, 41, 253, 211, 3, 42, 293, 17, 22, 10, 23, 36, 724, 50, 27, 17, 22, 23, 2, 1416, 58, 34, 128, 132, 15, 20, 62, 9, 1182, 252, 1, 16, 62, 36, 724, 50, 58, 41, 253, 45, 3, 220, 4, 41, 253, 211, 3, 42, 293, 293, 293, 17, 22, 10, 23, 36, 724, 50, 27, 17, 22, 23, 2, 1416, 58, 34, 128, 132, 15, 20, 62, 9, 1182, 252, 1, 16, 62, 36, 724, 50, 58, 41, 253, 45, 3, 220, 4, 41, 253, 211, 3, 42]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=30000)\n",
    "tokenizer.fit_on_texts(train_features)\n",
    "\n",
    "x_k_train = tokenizer.texts_to_sequences(train_features)\n",
    "x_k_test = tokenizer.texts_to_sequences(test_features)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "categories=list(train_targets.unique())\n",
    "\n",
    "#print(X_train[2])\n",
    "print(x_k_train[2])\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "x_k_train = pad_sequences(x_k_train, padding='post', maxlen=maxlen)\n",
    "x_k_test = pad_sequences(x_k_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mz:\\npr\\nlp-mc-1\\notebooks\\pytorch_models_train copy.ipynb Cell 4\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/npr/nlp-mc-1/notebooks/pytorch_models_train%20copy.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Convert the data into PyTorch tensors\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/z%3A/npr/nlp-mc-1/notebooks/pytorch_models_train%20copy.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_features_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(train_features\u001b[39m.\u001b[39;49mvalues, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/npr/nlp-mc-1/notebooks/pytorch_models_train%20copy.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_targets_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(train_targets\u001b[39m.\u001b[39mvalues, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/npr/nlp-mc-1/notebooks/pytorch_models_train%20copy.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m test_features_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(test_features\u001b[39m.\u001b[39mvalues, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "# Convert the data into PyTorch tensors\n",
    "train_features_tensor = torch.tensor(train_features.values, dtype=torch.float32)\n",
    "train_targets_tensor = torch.tensor(train_targets.values, dtype=torch.long)\n",
    "\n",
    "test_features_tensor = torch.tensor(test_features.values, dtype=torch.float32)\n",
    "test_targets_tensor = torch.tensor(test_targets.values, dtype=torch.long)\n",
    "\n",
    "# Combine the features and targets into PyTorch datasets\n",
    "train_dataset = TensorDataset(train_features_tensor, train_targets_tensor)\n",
    "test_dataset = TensorDataset(test_features_tensor, test_targets_tensor)\n",
    "\n",
    "# Create PyTorch dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 50)          2881300   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 50)          0         \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 50)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              52224     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                10250     \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,943,774\n",
      "Trainable params: 2,943,774\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mz:\\npr\\nlp-mc-1\\notebooks\\pytorch_models_train copy.ipynb Cell 5\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/z%3A/npr/nlp-mc-1/notebooks/pytorch_models_train%20copy.ipynb#X22sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[0;32m     <a href='vscode-notebook-cell:/z%3A/npr/nlp-mc-1/notebooks/pytorch_models_train%20copy.ipynb#X22sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m), loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/z%3A/npr/nlp-mc-1/notebooks/pytorch_models_train%20copy.ipynb#X22sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_k_train, y_k_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m120\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(x_k_test, y_k_test), verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\bruno\\anaconda3\\envs\\edap5\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\bruno\\anaconda3\\envs\\edap5\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\bruno\\anaconda3\\envs\\edap5\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\bruno\\anaconda3\\envs\\edap5\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\bruno\\anaconda3\\envs\\edap5\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:980\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    977\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    978\u001b[0m     \u001b[39m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    979\u001b[0m     \u001b[39m# stateless function.\u001b[39;00m\n\u001b[1;32m--> 980\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    981\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    982\u001b[0m   _, _, filtered_flat_args \u001b[39m=\u001b[39m (\n\u001b[0;32m    983\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn\u001b[39m.\u001b[39m_function_spec\u001b[39m.\u001b[39mcanonicalize_function_inputs(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    984\u001b[0m           \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n",
      "File \u001b[1;32mc:\\Users\\bruno\\anaconda3\\envs\\edap5\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\bruno\\anaconda3\\envs\\edap5\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\bruno\\anaconda3\\envs\\edap5\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\bruno\\anaconda3\\envs\\edap5\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from   tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "# https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "y_k_train = pd.get_dummies(train_targets)\n",
    "y_k_test = pd.get_dummies(test_targets)\n",
    "\n",
    "#initializer ='random_uniform'\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 50))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(1024, kernel_initializer='glorot_uniform', activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.Dense( len(categories), activation=tf.nn.sigmoid))\n",
    "model.add(keras.layers.Softmax())\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_k_train, y_k_train, epochs=120, batch_size=512, validation_data=(x_k_test, y_k_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_14 (Embedding)    (None, 200, 50)           2881300   \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 196, 128)          32128     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 98, 128)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 1024)              12846080  \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 10)                10250     \n",
      "                                                                 \n",
      " softmax_8 (Softmax)         (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,769,758\n",
      "Trainable params: 15,769,758\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/120\n",
      "38/38 [==============================] - 5s 19ms/step - loss: 0.3067 - accuracy: 0.2266 - val_loss: 0.3002 - val_accuracy: 0.2171\n",
      "Epoch 2/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2961 - accuracy: 0.2248 - val_loss: 0.2939 - val_accuracy: 0.2312\n",
      "Epoch 3/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2893 - accuracy: 0.2459 - val_loss: 0.2902 - val_accuracy: 0.2566\n",
      "Epoch 4/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2851 - accuracy: 0.2613 - val_loss: 0.2893 - val_accuracy: 0.2668\n",
      "Epoch 5/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2823 - accuracy: 0.2693 - val_loss: 0.2893 - val_accuracy: 0.2683\n",
      "Epoch 6/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2800 - accuracy: 0.2766 - val_loss: 0.2890 - val_accuracy: 0.2744\n",
      "Epoch 7/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2782 - accuracy: 0.2812 - val_loss: 0.2896 - val_accuracy: 0.2721\n",
      "Epoch 8/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2770 - accuracy: 0.2820 - val_loss: 0.2900 - val_accuracy: 0.2736\n",
      "Epoch 9/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2762 - accuracy: 0.2910 - val_loss: 0.2901 - val_accuracy: 0.2763\n",
      "Epoch 10/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2760 - accuracy: 0.2958 - val_loss: 0.2910 - val_accuracy: 0.2828\n",
      "Epoch 11/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2742 - accuracy: 0.3059 - val_loss: 0.2891 - val_accuracy: 0.2729\n",
      "Epoch 12/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2727 - accuracy: 0.3126 - val_loss: 0.2894 - val_accuracy: 0.2824\n",
      "Epoch 13/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2720 - accuracy: 0.3276 - val_loss: 0.2904 - val_accuracy: 0.2893\n",
      "Epoch 14/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2710 - accuracy: 0.3348 - val_loss: 0.2899 - val_accuracy: 0.2908\n",
      "Epoch 15/120\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.2701 - accuracy: 0.3520 - val_loss: 0.2901 - val_accuracy: 0.3030\n",
      "Epoch 16/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2691 - accuracy: 0.3521 - val_loss: 0.2899 - val_accuracy: 0.3030\n",
      "Epoch 17/120\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.2682 - accuracy: 0.3609 - val_loss: 0.2901 - val_accuracy: 0.3067\n",
      "Epoch 18/120\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.2676 - accuracy: 0.3675 - val_loss: 0.2899 - val_accuracy: 0.3095\n",
      "Epoch 19/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2670 - accuracy: 0.3772 - val_loss: 0.2905 - val_accuracy: 0.3028\n",
      "Epoch 20/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2662 - accuracy: 0.3810 - val_loss: 0.2922 - val_accuracy: 0.3074\n",
      "Epoch 21/120\n",
      "38/38 [==============================] - 1s 13ms/step - loss: 0.2657 - accuracy: 0.3865 - val_loss: 0.2913 - val_accuracy: 0.3109\n",
      "Epoch 22/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2652 - accuracy: 0.3868 - val_loss: 0.2923 - val_accuracy: 0.3128\n",
      "Epoch 23/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2649 - accuracy: 0.3891 - val_loss: 0.2918 - val_accuracy: 0.3101\n",
      "Epoch 24/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2645 - accuracy: 0.4027 - val_loss: 0.2918 - val_accuracy: 0.3174\n",
      "Epoch 25/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2643 - accuracy: 0.4100 - val_loss: 0.2929 - val_accuracy: 0.3137\n",
      "Epoch 26/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2641 - accuracy: 0.4085 - val_loss: 0.2923 - val_accuracy: 0.3126\n",
      "Epoch 27/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2633 - accuracy: 0.4071 - val_loss: 0.2918 - val_accuracy: 0.3164\n",
      "Epoch 28/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2623 - accuracy: 0.4184 - val_loss: 0.2921 - val_accuracy: 0.3195\n",
      "Epoch 29/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2613 - accuracy: 0.4216 - val_loss: 0.2921 - val_accuracy: 0.3210\n",
      "Epoch 30/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2607 - accuracy: 0.4369 - val_loss: 0.2922 - val_accuracy: 0.3244\n",
      "Epoch 31/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2602 - accuracy: 0.4923 - val_loss: 0.2928 - val_accuracy: 0.3321\n",
      "Epoch 32/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2596 - accuracy: 0.5436 - val_loss: 0.2949 - val_accuracy: 0.3225\n",
      "Epoch 33/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2587 - accuracy: 0.5575 - val_loss: 0.2943 - val_accuracy: 0.3275\n",
      "Epoch 34/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2581 - accuracy: 0.5671 - val_loss: 0.2956 - val_accuracy: 0.3246\n",
      "Epoch 35/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2571 - accuracy: 0.5739 - val_loss: 0.2944 - val_accuracy: 0.3290\n",
      "Epoch 36/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2563 - accuracy: 0.5890 - val_loss: 0.2949 - val_accuracy: 0.3319\n",
      "Epoch 37/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2554 - accuracy: 0.5991 - val_loss: 0.2960 - val_accuracy: 0.3282\n",
      "Epoch 38/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2547 - accuracy: 0.6060 - val_loss: 0.2950 - val_accuracy: 0.3359\n",
      "Epoch 39/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2540 - accuracy: 0.6127 - val_loss: 0.2951 - val_accuracy: 0.3349\n",
      "Epoch 40/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2534 - accuracy: 0.6238 - val_loss: 0.2960 - val_accuracy: 0.3363\n",
      "Epoch 41/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2531 - accuracy: 0.6253 - val_loss: 0.2959 - val_accuracy: 0.3321\n",
      "Epoch 42/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2527 - accuracy: 0.6285 - val_loss: 0.2959 - val_accuracy: 0.3393\n",
      "Epoch 43/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2522 - accuracy: 0.6416 - val_loss: 0.2959 - val_accuracy: 0.3403\n",
      "Epoch 44/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2511 - accuracy: 0.6526 - val_loss: 0.2954 - val_accuracy: 0.3401\n",
      "Epoch 45/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2501 - accuracy: 0.6645 - val_loss: 0.2949 - val_accuracy: 0.3418\n",
      "Epoch 46/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2498 - accuracy: 0.6657 - val_loss: 0.2953 - val_accuracy: 0.3426\n",
      "Epoch 47/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2496 - accuracy: 0.6727 - val_loss: 0.2956 - val_accuracy: 0.3456\n",
      "Epoch 48/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2492 - accuracy: 0.6776 - val_loss: 0.2965 - val_accuracy: 0.3466\n",
      "Epoch 49/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2489 - accuracy: 0.6769 - val_loss: 0.2958 - val_accuracy: 0.3420\n",
      "Epoch 50/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2487 - accuracy: 0.6866 - val_loss: 0.2959 - val_accuracy: 0.3456\n",
      "Epoch 51/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2485 - accuracy: 0.6916 - val_loss: 0.2963 - val_accuracy: 0.3475\n",
      "Epoch 52/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2484 - accuracy: 0.6958 - val_loss: 0.2964 - val_accuracy: 0.3460\n",
      "Epoch 53/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2483 - accuracy: 0.6966 - val_loss: 0.2965 - val_accuracy: 0.3454\n",
      "Epoch 54/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2482 - accuracy: 0.6972 - val_loss: 0.2968 - val_accuracy: 0.3433\n",
      "Epoch 55/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2480 - accuracy: 0.6966 - val_loss: 0.2971 - val_accuracy: 0.3426\n",
      "Epoch 56/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2480 - accuracy: 0.7017 - val_loss: 0.2972 - val_accuracy: 0.3479\n",
      "Epoch 57/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2479 - accuracy: 0.7095 - val_loss: 0.2974 - val_accuracy: 0.3494\n",
      "Epoch 58/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2478 - accuracy: 0.7106 - val_loss: 0.2967 - val_accuracy: 0.3460\n",
      "Epoch 59/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2478 - accuracy: 0.7118 - val_loss: 0.2970 - val_accuracy: 0.3456\n",
      "Epoch 60/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2478 - accuracy: 0.7139 - val_loss: 0.2970 - val_accuracy: 0.3491\n",
      "Epoch 61/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2477 - accuracy: 0.7122 - val_loss: 0.2973 - val_accuracy: 0.3416\n",
      "Epoch 62/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2478 - accuracy: 0.7204 - val_loss: 0.2963 - val_accuracy: 0.3504\n",
      "Epoch 63/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2475 - accuracy: 0.7172 - val_loss: 0.2968 - val_accuracy: 0.3485\n",
      "Epoch 64/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2474 - accuracy: 0.7251 - val_loss: 0.2971 - val_accuracy: 0.3531\n",
      "Epoch 65/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2471 - accuracy: 0.7342 - val_loss: 0.2967 - val_accuracy: 0.3554\n",
      "Epoch 66/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2469 - accuracy: 0.7389 - val_loss: 0.2975 - val_accuracy: 0.3552\n",
      "Epoch 67/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2465 - accuracy: 0.7431 - val_loss: 0.2974 - val_accuracy: 0.3529\n",
      "Epoch 68/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2464 - accuracy: 0.7448 - val_loss: 0.2977 - val_accuracy: 0.3548\n",
      "Epoch 69/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2461 - accuracy: 0.7503 - val_loss: 0.2966 - val_accuracy: 0.3609\n",
      "Epoch 70/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2457 - accuracy: 0.7538 - val_loss: 0.2973 - val_accuracy: 0.3578\n",
      "Epoch 71/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2457 - accuracy: 0.7593 - val_loss: 0.2972 - val_accuracy: 0.3605\n",
      "Epoch 72/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2454 - accuracy: 0.7621 - val_loss: 0.2973 - val_accuracy: 0.3601\n",
      "Epoch 73/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2451 - accuracy: 0.7682 - val_loss: 0.2977 - val_accuracy: 0.3590\n",
      "Epoch 74/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2449 - accuracy: 0.7714 - val_loss: 0.2977 - val_accuracy: 0.3563\n",
      "Epoch 75/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2447 - accuracy: 0.7744 - val_loss: 0.2974 - val_accuracy: 0.3567\n",
      "Epoch 76/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2445 - accuracy: 0.7782 - val_loss: 0.2984 - val_accuracy: 0.3588\n",
      "Epoch 77/120\n",
      "38/38 [==============================] - 0s 11ms/step - loss: 0.2444 - accuracy: 0.7829 - val_loss: 0.2994 - val_accuracy: 0.3561\n",
      "Epoch 78/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2442 - accuracy: 0.7915 - val_loss: 0.2988 - val_accuracy: 0.3586\n",
      "Epoch 79/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2440 - accuracy: 0.7921 - val_loss: 0.2991 - val_accuracy: 0.3613\n",
      "Epoch 80/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2438 - accuracy: 0.7979 - val_loss: 0.2990 - val_accuracy: 0.3605\n",
      "Epoch 81/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2435 - accuracy: 0.8026 - val_loss: 0.2994 - val_accuracy: 0.3590\n",
      "Epoch 82/120\n",
      "38/38 [==============================] - 0s 11ms/step - loss: 0.2435 - accuracy: 0.8030 - val_loss: 0.2978 - val_accuracy: 0.3643\n",
      "Epoch 83/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2431 - accuracy: 0.8074 - val_loss: 0.2986 - val_accuracy: 0.3693\n",
      "Epoch 84/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2430 - accuracy: 0.8139 - val_loss: 0.2980 - val_accuracy: 0.3708\n",
      "Epoch 85/120\n",
      "38/38 [==============================] - 0s 11ms/step - loss: 0.2429 - accuracy: 0.8147 - val_loss: 0.2989 - val_accuracy: 0.3651\n",
      "Epoch 86/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2427 - accuracy: 0.8172 - val_loss: 0.2987 - val_accuracy: 0.3672\n",
      "Epoch 87/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2426 - accuracy: 0.8231 - val_loss: 0.2994 - val_accuracy: 0.3676\n",
      "Epoch 88/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2424 - accuracy: 0.8252 - val_loss: 0.2985 - val_accuracy: 0.3722\n",
      "Epoch 89/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2423 - accuracy: 0.8243 - val_loss: 0.2984 - val_accuracy: 0.3653\n",
      "Epoch 90/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2423 - accuracy: 0.8278 - val_loss: 0.2986 - val_accuracy: 0.3697\n",
      "Epoch 91/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2421 - accuracy: 0.8293 - val_loss: 0.2997 - val_accuracy: 0.3674\n",
      "Epoch 92/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2421 - accuracy: 0.8317 - val_loss: 0.2999 - val_accuracy: 0.3651\n",
      "Epoch 93/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2419 - accuracy: 0.8329 - val_loss: 0.2996 - val_accuracy: 0.3699\n",
      "Epoch 94/120\n",
      "38/38 [==============================] - 0s 11ms/step - loss: 0.2418 - accuracy: 0.8372 - val_loss: 0.2992 - val_accuracy: 0.3708\n",
      "Epoch 95/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2417 - accuracy: 0.8392 - val_loss: 0.2995 - val_accuracy: 0.3683\n",
      "Epoch 96/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2417 - accuracy: 0.8373 - val_loss: 0.2999 - val_accuracy: 0.3670\n",
      "Epoch 97/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2416 - accuracy: 0.8395 - val_loss: 0.2998 - val_accuracy: 0.3693\n",
      "Epoch 98/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2415 - accuracy: 0.8413 - val_loss: 0.3001 - val_accuracy: 0.3687\n",
      "Epoch 99/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2414 - accuracy: 0.8423 - val_loss: 0.3001 - val_accuracy: 0.3706\n",
      "Epoch 100/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2414 - accuracy: 0.8438 - val_loss: 0.3004 - val_accuracy: 0.3655\n",
      "Epoch 101/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2413 - accuracy: 0.8454 - val_loss: 0.3009 - val_accuracy: 0.3662\n",
      "Epoch 102/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2413 - accuracy: 0.8462 - val_loss: 0.3004 - val_accuracy: 0.3676\n",
      "Epoch 103/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2413 - accuracy: 0.8473 - val_loss: 0.2999 - val_accuracy: 0.3632\n",
      "Epoch 104/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2412 - accuracy: 0.8484 - val_loss: 0.3004 - val_accuracy: 0.3647\n",
      "Epoch 105/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2412 - accuracy: 0.8498 - val_loss: 0.3001 - val_accuracy: 0.3638\n",
      "Epoch 106/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2411 - accuracy: 0.8518 - val_loss: 0.3000 - val_accuracy: 0.3643\n",
      "Epoch 107/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2411 - accuracy: 0.8520 - val_loss: 0.2998 - val_accuracy: 0.3594\n",
      "Epoch 108/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2410 - accuracy: 0.8531 - val_loss: 0.3009 - val_accuracy: 0.3622\n",
      "Epoch 109/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2410 - accuracy: 0.8550 - val_loss: 0.3007 - val_accuracy: 0.3578\n",
      "Epoch 110/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2409 - accuracy: 0.8539 - val_loss: 0.3003 - val_accuracy: 0.3674\n",
      "Epoch 111/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2409 - accuracy: 0.8551 - val_loss: 0.3000 - val_accuracy: 0.3657\n",
      "Epoch 112/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2408 - accuracy: 0.8566 - val_loss: 0.3003 - val_accuracy: 0.3657\n",
      "Epoch 113/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2407 - accuracy: 0.8570 - val_loss: 0.3011 - val_accuracy: 0.3687\n",
      "Epoch 114/120\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2407 - accuracy: 0.8576 - val_loss: 0.3009 - val_accuracy: 0.3620\n",
      "Epoch 115/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2407 - accuracy: 0.8582 - val_loss: 0.3009 - val_accuracy: 0.3611\n",
      "Epoch 116/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2407 - accuracy: 0.8586 - val_loss: 0.3005 - val_accuracy: 0.3659\n",
      "Epoch 117/120\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.2406 - accuracy: 0.8593 - val_loss: 0.3009 - val_accuracy: 0.3643\n",
      "Epoch 118/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2407 - accuracy: 0.8612 - val_loss: 0.3008 - val_accuracy: 0.3664\n",
      "Epoch 119/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2406 - accuracy: 0.8607 - val_loss: 0.3010 - val_accuracy: 0.3609\n",
      "Epoch 120/120\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2406 - accuracy: 0.8616 - val_loss: 0.3009 - val_accuracy: 0.3628\n"
     ]
    }
   ],
   "source": [
    "#initializer ='random_uniform'\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 50, input_length=200))\n",
    "model.add(keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.Dense(1024, kernel_initializer='glorot_uniform', activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.Dense( len(categories), activation=tf.nn.sigmoid))\n",
    "model.add(keras.layers.Softmax())\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_k_train, y_k_train, epochs=120, batch_size=512, validation_data=(x_k_test, y_k_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074021e6530e46b5b17918759d488cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0e663537b34e8896e5d4a192dfb973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6eb255cdb54a8e92507ee87968256b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2c2c6d807e4865a5486f2c4262c0d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ec61f372704222970bfa2fb82e4a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b44268f0ec743549f2ac8c9369eeda1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/84.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac244ead3924bdca7d928b14fa09426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331bfbc447a745b3ad4b7a6963b61b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc158b66624b4988ab2279808ab26b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.21M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a885e5d5a74233b84ab45c6ca4aa0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3457b00a2bc4bd68f9806e6abf8adb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/383k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf1194c42f1434f89597ef22cfb8bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\bruno/.cache\\\\torch\\\\sentence_transformers\\\\brunokreiner_lyrics-bert\\\\1_Pooling\\\\config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mz:\\npr\\nlp-mc-1\\notebooks\\pytorch_models_train copy.ipynb Cell 7\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/npr/nlp-mc-1/notebooks/pytorch_models_train%20copy.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msentence_transformers\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m----> <a href='vscode-notebook-cell:/z%3A/npr/nlp-mc-1/notebooks/pytorch_models_train%20copy.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m transformer_model \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39m'\u001b[39;49m\u001b[39mbrunokreiner/lyrics-bert\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\bruno\\anaconda3\\envs\\edap5\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:95\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[0;32m     87\u001b[0m         snapshot_download(model_name_or_path,\n\u001b[0;32m     88\u001b[0m                             cache_dir\u001b[39m=\u001b[39mcache_folder,\n\u001b[0;32m     89\u001b[0m                             library_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msentence-transformers\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     90\u001b[0m                             library_version\u001b[39m=\u001b[39m__version__,\n\u001b[0;32m     91\u001b[0m                             ignore_files\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mflax_model.msgpack\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrust_model.ot\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtf_model.h5\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     92\u001b[0m                             use_auth_token\u001b[39m=\u001b[39muse_auth_token)\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_path, \u001b[39m'\u001b[39m\u001b[39mmodules.json\u001b[39m\u001b[39m'\u001b[39m)):    \u001b[39m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_sbert_model(model_path)\n\u001b[0;32m     96\u001b[0m \u001b[39melse\u001b[39;00m:   \u001b[39m#Load with AutoModel\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_auto_model(model_path)\n",
      "File \u001b[1;32mc:\\Users\\bruno\\anaconda3\\envs\\edap5\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:840\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[1;34m(self, model_path)\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[39mfor\u001b[39;00m module_config \u001b[39min\u001b[39;00m modules_config:\n\u001b[0;32m    839\u001b[0m     module_class \u001b[39m=\u001b[39m import_from_string(module_config[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 840\u001b[0m     module \u001b[39m=\u001b[39m module_class\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(model_path, module_config[\u001b[39m'\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[0;32m    841\u001b[0m     modules[module_config[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m module\n\u001b[0;32m    843\u001b[0m \u001b[39mreturn\u001b[39;00m modules\n",
      "File \u001b[1;32mc:\\Users\\bruno\\anaconda3\\envs\\edap5\\lib\\site-packages\\sentence_transformers\\models\\Pooling.py:117\u001b[0m, in \u001b[0;36mPooling.load\u001b[1;34m(input_path)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(input_path):\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(input_path, \u001b[39m'\u001b[39;49m\u001b[39mconfig.json\u001b[39;49m\u001b[39m'\u001b[39;49m)) \u001b[39mas\u001b[39;00m fIn:\n\u001b[0;32m    118\u001b[0m         config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(fIn)\n\u001b[0;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m Pooling(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\bruno/.cache\\\\torch\\\\sentence_transformers\\\\brunokreiner_lyrics-bert\\\\1_Pooling\\\\config.json'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "transformer_model = SentenceTransformer('brunokreiner/lyrics-bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\bruno/.cache\\\\torch\\\\sentence_transformers\\\\brunokreiner_lyrics-bert\\\\./1_Pooling\\\\config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mz:\\npr\\nlp-mc-1\\notebooks\\pytorch_models_train copy.ipynb Cell 8\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/npr/nlp-mc-1/notebooks/pytorch_models_train%20copy.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msentence_transformers\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m----> <a href='vscode-notebook-cell:/z%3A/npr/nlp-mc-1/notebooks/pytorch_models_train%20copy.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39m\"\u001b[39;49m\u001b[39mbrunokreiner/lyrics-bert\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\bruno\\anaconda3\\envs\\edap5\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:95\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[0;32m     87\u001b[0m         snapshot_download(model_name_or_path,\n\u001b[0;32m     88\u001b[0m                             cache_dir\u001b[39m=\u001b[39mcache_folder,\n\u001b[0;32m     89\u001b[0m                             library_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msentence-transformers\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     90\u001b[0m                             library_version\u001b[39m=\u001b[39m__version__,\n\u001b[0;32m     91\u001b[0m                             ignore_files\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mflax_model.msgpack\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrust_model.ot\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtf_model.h5\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     92\u001b[0m                             use_auth_token\u001b[39m=\u001b[39muse_auth_token)\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_path, \u001b[39m'\u001b[39m\u001b[39mmodules.json\u001b[39m\u001b[39m'\u001b[39m)):    \u001b[39m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_sbert_model(model_path)\n\u001b[0;32m     96\u001b[0m \u001b[39melse\u001b[39;00m:   \u001b[39m#Load with AutoModel\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_auto_model(model_path)\n",
      "File \u001b[1;32mc:\\Users\\bruno\\anaconda3\\envs\\edap5\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:840\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[1;34m(self, model_path)\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[39mfor\u001b[39;00m module_config \u001b[39min\u001b[39;00m modules_config:\n\u001b[0;32m    839\u001b[0m     module_class \u001b[39m=\u001b[39m import_from_string(module_config[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 840\u001b[0m     module \u001b[39m=\u001b[39m module_class\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(model_path, module_config[\u001b[39m'\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[0;32m    841\u001b[0m     modules[module_config[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m module\n\u001b[0;32m    843\u001b[0m \u001b[39mreturn\u001b[39;00m modules\n",
      "File \u001b[1;32mc:\\Users\\bruno\\anaconda3\\envs\\edap5\\lib\\site-packages\\sentence_transformers\\models\\Pooling.py:117\u001b[0m, in \u001b[0;36mPooling.load\u001b[1;34m(input_path)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(input_path):\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(input_path, \u001b[39m'\u001b[39;49m\u001b[39mconfig.json\u001b[39;49m\u001b[39m'\u001b[39;49m)) \u001b[39mas\u001b[39;00m fIn:\n\u001b[0;32m    118\u001b[0m         config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(fIn)\n\u001b[0;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m Pooling(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\bruno/.cache\\\\torch\\\\sentence_transformers\\\\brunokreiner_lyrics-bert\\\\./1_Pooling\\\\config.json'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"brunokreiner/lyrics-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_k_train = tokenizer.texts_to_sequences(train_features)\n",
    "x_k_test = tokenizer.texts_to_sequences(test_features)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "categories=list(train_targets.unique())\n",
    "\n",
    "#print(X_train[2])\n",
    "print(x_k_train[2])\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "x_k_train = pad_sequences(x_k_train, padding='post', maxlen=maxlen)\n",
    "x_k_test = pad_sequences(x_k_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "#initializer ='random_uniform'\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 50, input_length=200))\n",
    "model.add(keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.Dense(1024, kernel_initializer='glorot_uniform', activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.Dense( len(categories), activation=tf.nn.sigmoid))\n",
    "model.add(keras.layers.Softmax())\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_k_train, y_k_train, epochs=120, batch_size=512, validation_data=(x_k_test, y_k_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyModel1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Define your PyTorch models here\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m models \u001b[39m=\u001b[39m [MyModel1(), MyModel2(), MyModel3()]\n\u001b[0;32m      4\u001b[0m \u001b[39m# Define your loss function\u001b[39;00m\n\u001b[0;32m      5\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MyModel1' is not defined"
     ]
    }
   ],
   "source": [
    "# Define your PyTorch models here\n",
    "models = [MyModel1(), MyModel2(), MyModel3()]\n",
    "\n",
    "# Define your loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create a list to store the optimizer for each model\n",
    "optimizers = [optim.Adam(model.parameters(), lr=0.001) for model in models]\n",
    "\n",
    "# Create a dataframe to store the model accuracy, train loss, and test loss\n",
    "df = pd.DataFrame(columns=['model', 'epoch', 'accuracy', 'train_loss', 'test_loss'])\n",
    "\n",
    "# Train your models\n",
    "num_epochs = 10\n",
    "\n",
    "for model_idx, model in enumerate(models):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train the model\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizers[model_idx].zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizers[model_idx].step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_dataloader:\n",
    "                outputs = model(inputs)\n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "                total_correct += (predictions == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "        accuracy = total_correct / total_samples\n",
    "        train_loss = train_loss / len(train_dataloader.dataset)\n",
    "        test_loss = test_loss / len(test_dataloader.dataset)\n",
    "        print(f\"Model {model_idx+1} - Epoch {epoch+1} Accuracy: {accuracy:.4f} Train Loss: {train_loss:.4f} Test Loss: {test_loss:.4f}\")\n",
    "        \n",
    "        # Add the accuracy, train loss, and test loss to the dataframe\n",
    "        df = df.append({'model': f'model{model_idx+1}', 'epoch': epoch+1, 'accuracy': accuracy, 'train_loss': train_loss, 'test_loss': test_loss}, ignore_index=True)\n",
    "\n",
    "    # Save the dataframe as a CSV file for each model\n",
    "    df.to_csv(f'model{model_idx+1}_accuracy.csv', index=False)\n",
    "    df = df.iloc[0:0]\n",
    "\n",
    "    # Generate the confusion matrix for the model on the test data\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predictions.cpu().numpy())\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    print(f\"Model {model_idx+1} Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(f\"Classification Report:\")\n",
    "    print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npr-mc1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
