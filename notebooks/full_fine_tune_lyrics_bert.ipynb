{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../models/lyrics-bert/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/media/bruno/0d2f61d2-2b9c-4043-9a46-8e4dfe74fc95/bruno/anaconda3/envs/nlp-mc1/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       id                                             lyrics  \\\n",
      "0  1FAmKoufyAXMfzPPs9bsjA  i tied my bandana took my pack from the floor ...   \n",
      "1  3QvPEv8XjHa73iYhaienWw  i want to live on the moon never see a human a...   \n",
      "2  5VPFATm85G3P04Q5g8yxqr  bitch you know you can t parallel park anyway ...   \n",
      "3  7J2jCftItt7htcOUdcMnpt  graceless falling slipping in the cold with no...   \n",
      "4  4cBPzVIbDIQx0LIyauFAy0  madame morse estate stood five hundred years p...   \n",
      "\n",
      "       artist_name most_common_genre  \\\n",
      "0  Waylon Jennings           country   \n",
      "1   Phantom Planet              rock   \n",
      "2    Isaiah Rashad           hip-hop   \n",
      "3     Matt Pond PA             indie   \n",
      "4       Ariel Pink               pop   \n",
      "\n",
      "                                          genre_list  \n",
      "0  ['country', 'country', 'rock', 'outlaw', 'coun...  \n",
      "1                                    ['pop', 'rock']  \n",
      "2  ['hip-hop', 'rap', 'tennessee', 'hip-hop', 'un...  \n",
      "3                                ['philly', 'indie']  \n",
      "4  ['art', 'pop', 'chillwave', 'dream', 'pop', 'e...  \n",
      "The model has 20989654 trainable parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:33<00:00,  2.23it/s]\n",
      "Evaluating: 100%|██████████| 19/19 [00:04<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Accuracy: 0.2114 Test Accuracy: 0.2337 F1 Score: 0.0885 Train Loss: 2.1918 Test Loss: 2.1423 Train Time/Step: 0.4475 Eval Time/Step: 0.2171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:34<00:00,  2.19it/s]\n",
      "Evaluating: 100%|██████████| 19/19 [00:04<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train Accuracy: 0.2361 Test Accuracy: 0.2337 F1 Score: 0.0885 Train Loss: 2.1370 Test Loss: 2.1415 Train Time/Step: 0.4559 Eval Time/Step: 0.2173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:34<00:00,  2.16it/s]\n",
      "Evaluating: 100%|██████████| 19/19 [00:04<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train Accuracy: 0.2362 Test Accuracy: 0.2337 F1 Score: 0.0885 Train Loss: 2.1355 Test Loss: 2.1418 Train Time/Step: 0.4624 Eval Time/Step: 0.2205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:34<00:00,  2.19it/s]\n",
      "Evaluating: 100%|██████████| 19/19 [00:04<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train Accuracy: 0.2368 Test Accuracy: 0.2337 F1 Score: 0.0885 Train Loss: 2.1353 Test Loss: 2.1421 Train Time/Step: 0.4556 Eval Time/Step: 0.2171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:34<00:00,  2.19it/s]\n",
      "Evaluating: 100%|██████████| 19/19 [00:04<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train Accuracy: 0.2361 Test Accuracy: 0.2337 F1 Score: 0.0885 Train Loss: 2.1360 Test Loss: 2.1419 Train Time/Step: 0.4571 Eval Time/Step: 0.2218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:34<00:00,  2.20it/s]\n",
      "Evaluating: 100%|██████████| 19/19 [00:04<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Train Accuracy: 0.2362 Test Accuracy: 0.2337 F1 Score: 0.0885 Train Loss: 2.1354 Test Loss: 2.1419 Train Time/Step: 0.4552 Eval Time/Step: 0.2227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:33<00:00,  2.22it/s]\n",
      "Evaluating: 100%|██████████| 19/19 [00:03<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Train Accuracy: 0.2372 Test Accuracy: 0.2337 F1 Score: 0.0885 Train Loss: 2.1351 Test Loss: 2.1418 Train Time/Step: 0.4502 Eval Time/Step: 0.1998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:34<00:00,  2.18it/s]\n",
      "Evaluating: 100%|██████████| 19/19 [00:04<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Train Accuracy: 0.2367 Test Accuracy: 0.2337 F1 Score: 0.0885 Train Loss: 2.1331 Test Loss: 2.1419 Train Time/Step: 0.4595 Eval Time/Step: 0.2222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:33<00:00,  2.24it/s]\n",
      "Evaluating: 100%|██████████| 19/19 [00:03<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Train Accuracy: 0.2366 Test Accuracy: 0.2337 F1 Score: 0.0885 Train Loss: 2.1348 Test Loss: 2.1417 Train Time/Step: 0.4464 Eval Time/Step: 0.2081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:34<00:00,  2.15it/s]\n",
      "Evaluating: 100%|██████████| 19/19 [00:04<00:00,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Train Accuracy: 0.2374 Test Accuracy: 0.2337 F1 Score: 0.0885 Train Loss: 2.1350 Test Loss: 2.1418 Train Time/Step: 0.4644 Eval Time/Step: 0.2149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizerFast, AdamW\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import nlpaug.augmenter.word as naw\n",
    "import random\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "# Load train and test data\n",
    "train_data = pd.read_csv(\"../data/train_data.csv\")\n",
    "test_data = pd.read_csv(\"../data/test_data.csv\")\n",
    "\n",
    "print(train_data.head())\n",
    "\n",
    "# Split train and test data into features and targets\n",
    "train_features = train_data[\"lyrics\"]\n",
    "train_targets = train_data[\"most_common_genre\"]\n",
    "\n",
    "test_features = test_data[\"lyrics\"]\n",
    "test_targets = test_data[\"most_common_genre\"]\n",
    "\n",
    "def model_summary(model):\n",
    "    print(\"Model summary:\")\n",
    "    print(\"---------------------------\")\n",
    "    total_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        param_count = param.numel()\n",
    "        total_params += param_count\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "\n",
    "categories = sorted(list(train_targets.unique()))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, data, labels, augment=False, augmentation_rate=0.1):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "        self.augmentation_rate = augmentation_rate\n",
    "        self.category_to_index = {category: index for index, category in enumerate(categories)}\n",
    "        \n",
    "        self.augmenter = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        lyrics = self.data.iloc[index]\n",
    "        label = self.labels.iloc[index]\n",
    "        label_index = torch.tensor(self.category_to_index[label], dtype=torch.long).to(device)\n",
    "\n",
    "        if self.augment:\n",
    "            lyrics = self.augmenter.augment(lyrics)\n",
    "            \n",
    "        return lyrics, label_index\n",
    "\n",
    "batchsize = 256\n",
    "learning_rate = 1e-5\n",
    "train_dataset = LyricsDataset(train_features, train_targets)\n",
    "test_dataset = LyricsDataset(test_features, test_targets)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight = \"balanced\",\n",
    "    classes = np.unique(train_targets),\n",
    "    y = train_targets  \n",
    ")\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"../models/lyrics-bert/\", num_labels=len(train_dataset.category_to_index))\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"../models/lyrics-bert/\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Metrics to store\n",
    "metrics = {\n",
    "    \"train_accuracy\": [],\n",
    "    \"test_accuracy\": [],\n",
    "    \"f1_score\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"test_loss\": [],\n",
    "    \"train_time_per_step\": [],\n",
    "    \"eval_time_per_step\": []\n",
    "}\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"The model has {trainable_params} trainable parameters.\")\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader)*10)\n",
    "\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    start_time = time.time()\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    correct_train_preds = 0\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        b_input_ids, b_labels = batch\n",
    "        b_input_ids = tokenizer(b_input_ids, padding=True, truncation=True, max_length=512, return_tensors='pt').input_ids.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(b_input_ids, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "        pred = torch.argmax(outputs.logits, dim=1)\n",
    "        correct_train_preds += (pred == b_labels).sum().item()\n",
    "        loss.backward()\n",
    "        # Gradient clipping to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_acc = correct_train_preds / len(train_dataset)\n",
    "    metrics[\"train_loss\"].append(avg_train_loss)\n",
    "    metrics[\"train_accuracy\"].append(train_acc)\n",
    "\n",
    "    # Time per step\n",
    "    train_time = time.time() - start_time\n",
    "    train_time_per_step = train_time / len(train_dataloader)\n",
    "    metrics[\"train_time_per_step\"].append(train_time_per_step)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    correct_test_preds = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            b_input_ids, b_labels = batch\n",
    "            b_input_ids = tokenizer(b_input_ids, padding=True, truncation=True, max_length=512, return_tensors='pt').input_ids.to(device)\n",
    "            outputs = model(b_input_ids, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            total_eval_loss += loss.item()\n",
    "            pred = torch.argmax(outputs.logits, dim=1)\n",
    "            correct_test_preds += (pred == b_labels).sum().item()\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(b_labels.cpu().numpy())\n",
    "\n",
    "    avg_test_loss = total_eval_loss / len(test_dataloader)\n",
    "    test_acc = correct_test_preds / len(test_dataset)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    metrics[\"test_loss\"].append(avg_test_loss)\n",
    "    metrics[\"test_accuracy\"].append(test_acc)\n",
    "    metrics[\"f1_score\"].append(f1)\n",
    "\n",
    "    # Time per step\n",
    "    eval_time = time.time() - start_time\n",
    "    eval_time_per_step = eval_time / len(test_dataloader)\n",
    "    metrics[\"eval_time_per_step\"].append(eval_time_per_step)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1} Train Accuracy: {train_acc:.4f} Test Accuracy: {test_acc:.4f} F1 Score: {f1:.4f} Train Loss: {avg_train_loss:.4f} Test Loss: {avg_test_loss:.4f} Train Time/Step: {train_time_per_step:.4f} Eval Time/Step: {eval_time_per_step:.4f}\")\n",
    "\n",
    "\n",
    "# Save metrics to CSV\n",
    "# Save metrics to CSV\n",
    "df = pd.DataFrame(metrics)\n",
    "df.to_csv(\"full-fine-tuned-lyrics-bert.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-mc1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
